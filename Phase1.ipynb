{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "notify_time": "5",
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "group21_Phase1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2d4395c702e040bbb8e69d0e7d7a9aee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_fc9c06978742439585f0332a2d2c2f0a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a6af3879353b4ba5b134fa7fb28e6272",
              "IPY_MODEL_093427a057984477801baddd526f23d7"
            ]
          }
        },
        "fc9c06978742439585f0332a2d2c2f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a6af3879353b4ba5b134fa7fb28e6272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_d15df66a2a584dd38fbdfbc11e43e579",
            "_dom_classes": [],
            "description": "  0%",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 100,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 0,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a891a5af58d2437bbd0db564ede7b94a"
          }
        },
        "093427a057984477801baddd526f23d7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f089289a774a40e4a5d7651a20b15b77",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0/100 [00:00&lt;?, ?it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85a34d4be9e84e708f0c151fdf069ac1"
          }
        },
        "d15df66a2a584dd38fbdfbc11e43e579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a891a5af58d2437bbd0db564ede7b94a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f089289a774a40e4a5d7651a20b15b77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85a34d4be9e84e708f0c151fdf069ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoenEtdlC58d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpn9k3BV1X2s"
      },
      "source": [
        "# CS331 - Spring 2021 - Phase 1 [15%]\n",
        "\n",
        "*__Submission Guidelines:__*\n",
        "- Naming convention for submission of this notebook is `groupXX_Phase1.ipynb` where XX needs to be replaced by your group number. For example: group 1 would rename their notebook to `group01_Phase1.ipynb`\n",
        "- Only the group lead is supposed to make the submission\n",
        "- All the cells <b>must</b> be run once before submission. If your submission's cells are not showing the results (plots etc.), marks wil be deducted\n",
        "- Only the code written within this notebook will be considered while grading. No other files will be entertained\n",
        "- You are advised to follow good programming practies including approriate variable naming and making use of logical comments \n",
        "\n",
        "The university honor code should be maintained. Any violation, if found, will result in disciplinary action. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9cjup_91X2_"
      },
      "source": [
        "#### <b>Introduction</b> \n",
        "This is the first of the three phases of this offering's project. To give an overview of this phase, we will essentially be building everything from scratch. The dataset that we will be using for this project is Fashion_MNIST dataset. This dataset consists of 70,000 images of fashion/clothing items belonging to 10 different categories/classes. It has furhter been divided into 60,000 training images and 10,000 test images and each image is a 28*28 grayscale image (hence 1 color channel). It is recommended that you go through  [this link](https://www.kaggle.com/zalando-research/fashionmnist) to familiarize yourself with the dataset\n",
        "\n",
        "You will begin by manually loading the dataset in this notebook (more instructions on this will follow) followed by from-scratch implementation of a Neural Netowrk (NN). Once done, you will have to tweak the hyperparameters (such as learning rate, number of epochs etc.) to get the best results for your NN's implementation\n",
        "\n",
        "###### <b>You will strictly be using for-loops fort this phase's implementation of NN (unless specified otherwise in the sub-section)\n",
        "\n",
        "###### Modification of the provided code without prior discussion with the TAs will result in a grade deduction</b>\n",
        "\n",
        "---\n",
        "\n",
        "###### <b>Side note</b>\n",
        "The `plot_model` method will only work if you have the `pydot` python package installed along with [Graphviz](https://graphviz.gitlab.io/download/). If you do not wish to use this then simply comment out the import for `pydot`\n",
        "\n",
        "###### <b>Need Help?</b>\n",
        "If you need help, please refer to the course staff ASAP and do not wait till the last moment as they might not be available on very short notice close to deadlines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5m1T5nJ6WH3"
      },
      "source": [
        "#### <b>Before You Begin</b>\n",
        "\n",
        "Skeleton code is provided to get you started. The main methods that you need to implement correspond to the four steps of the training process of a NN which are as follows:\n",
        "1. Initialize variables and initialize weights\n",
        "2. Forward pass\n",
        "3. Backward pass AKA Backpropogation\n",
        "4. Weight Update AKA Gradient Descent\n",
        "\n",
        "__Look for comments in the code to see where you are supposed to write your code__ \n",
        "\n",
        "A `fit` function is what combines the previous three functions and overall trains the network to __fit__ to the provided training examples. The provided `fit` methods requires all the four steps of the training process to be working correctly. The function has been setup in a way that it expects the above four methods to take particular inputs and return particular outputs. __You are supposed to work within this restriction__ \n",
        "\n",
        "\n",
        "\n",
        "__To see if your model is working correctly, you need to make sure that your model loss is going down during training__\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-30T14:39:17.671980Z",
          "start_time": "2019-01-30T14:38:40.137027Z"
        },
        "id": "UPL8Ut1u1X2-"
      },
      "source": [
        "# making all the necessary imports here\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg # New import added for extraction\n",
        "plt.style.use('seaborn')\n",
        "from IPython.display import Image\n",
        "import pydot\n",
        "from tqdm import tqdm_notebook\n",
        "import seaborn as sns\n",
        "from keras.datasets import fashion_mnist\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import np_utils\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix,classification_report\n",
        "from google.colab import drive\n",
        "import glob\n",
        "import cv2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-y-6i2TL1X3C"
      },
      "source": [
        "# This fucntion will be used to plot the confusion matrix at the end of this notebook\n",
        "\n",
        "def plot_confusion_matrix(conf_mat):\n",
        "    classes = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n",
        "    df_cm = pd.DataFrame(conf_mat,classes,classes)\n",
        "    plt.figure(figsize=(15,9))\n",
        "    sns.set(font_scale=1.4)\n",
        "    sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})\n",
        "    plt.show()\n",
        "\n",
        "class_labels = ['T-shirt/top','Trouser/pants','Pullover shirt','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-30T14:36:37.383767Z",
          "start_time": "2019-01-30T14:36:37.333537Z"
        },
        "id": "xU_yqbc31X2_"
      },
      "source": [
        "# Enter group lead's roll number here. This will be used for plotting purposes\n",
        "\n",
        "rollnumber = 22100282"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xHrz4X069_6D"
      },
      "source": [
        "#### __Read dataset__\n",
        "\n",
        "Get paths for all the training and test images in the dataset and print the length of training and test paths' list. For this purpose you can use glob. You can have a look [here](https://www.geeksforgeeks.org/how-to-use-glob-function-to-find-files-recursively-in-python/) on how to use glob. The dataset that has been provided to you guys is a truncated version of the Fashion MNIST dataset (having 2000 training images and 1600 test images, only)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g1jPQ5xaTlj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60238616-60a7-4ad2-9154-8e4b6ba57f36"
      },
      "source": [
        "# Mounting Google Drive here\n",
        "drive.mount('/drive')\n",
        "\n",
        "# Edit this address so that it points to the dataset's zipped file on your Google Drive\n",
        "!unzip -o -q \"/drive/MyDrive/Project/dataset.zip\" -d \"/content/data/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBj9_sg9-Auu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67116db-fdb6-45fa-d48a-c00e2b505e0f"
      },
      "source": [
        "classes = 10  # do not change this\n",
        "X_train = None  # you must store the training images in this varaible \n",
        "y_train = None  # you must store the training images' labels in this varaible\n",
        "X_test = None   # you must store the test images in this varaible\n",
        "y_test = None   # you must store the test images' labels in this varaible\n",
        "\n",
        "###### Code Here ######\n",
        "'''Please note that you will have to extract and one-hot encode the labels of the images for both y_train and y_test'''\n",
        "\n",
        "# TODO: The pixels already seem normalized to me when I outputted it but I am still not sure\n",
        "\n",
        "# --------------Have rigorously tested using for loops that each pixel is normalized in both the X_train and X_test datasets.\n",
        "\n",
        "# test_path = \"/content/data/test\"\n",
        "# train_path = \"/content/data/train\"\n",
        "\n",
        "\n",
        "test_paths = []\n",
        "train_paths = []\n",
        "\n",
        "X_train = []\n",
        "X_test = []\n",
        "y_train = []\n",
        "y_test = []\n",
        "\n",
        "for name in glob.glob(\"/content/data/train/*/*\"):\n",
        "  train_paths.append(name)\n",
        "\n",
        "for name in glob.glob(\"/content/data/test/*/*\"):\n",
        "  test_paths.append(name)\n",
        "\n",
        "\n",
        "\n",
        "for train_path in train_paths:\n",
        "  X_train.append(mpimg.imread(train_path))\n",
        "\n",
        "for test_path in test_paths:\n",
        "  X_test.append(mpimg.imread(test_path))\n",
        "\n",
        "# One hot encoding the data\n",
        "\n",
        "# --------------Tuples were causing issues with comparison operator ==. They would return a vector of True/False, rather than just one bool value.\n",
        "\n",
        "for train_path in train_paths:\n",
        "\n",
        "  if train_path.find(\"anklefoot\") != -1:\n",
        "    y_train.append([1,0,0,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"bag\") != -1:\n",
        "    y_train.append([0,1,0,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"coat\") != -1:\n",
        "    y_train.append([0,0,1,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"dress\") != -1:\n",
        "    y_train.append([0,0,0,1,0,0,0,0,0,0])\n",
        "\n",
        "\n",
        "  elif train_path.find(\"pants\") != -1:\n",
        "    y_train.append([0,0,0,0,1,0,0,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"pullovershirt\") != -1:\n",
        "    y_train.append([0,0,0,0,0,1,0,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"sandal\") != -1:\n",
        "    y_train.append([0,0,0,0,0,0,1,0,0,0])\n",
        "\n",
        "  elif train_path.find(\"shirt\") != -1:\n",
        "    y_train.append([0,0,0,0,0,0,0,1,0,0])\n",
        "\n",
        "  elif train_path.find(\"sneaker\") != -1:\n",
        "    y_train.append([0,0,0,0,0,0,0,0,1,0])\n",
        "\n",
        "  elif train_path.find(\"top\") != -1:\n",
        "    y_train.append([0,0,0,0,0,0,0,0,0,1])\n",
        "  else:\n",
        "    print(\"SOMETHING WENT WRONG!\")\n",
        "\n",
        "\n",
        "\n",
        "for test_path in test_paths:\n",
        "\n",
        "  if test_path.find(\"anklefoot\") != -1:\n",
        "    y_test.append([1,0,0,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"bag\") != -1:\n",
        "    y_test.append([0,1,0,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"coat\") != -1:\n",
        "    y_test.append([0,0,1,0,0,0,0,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"dress\") != -1:\n",
        "    y_test.append([0,0,0,1,0,0,0,0,0,0])\n",
        "\n",
        "\n",
        "  elif test_path.find(\"pants\") != -1:\n",
        "    y_test.append([0,0,0,0,1,0,0,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"pullovershirt\") != -1:\n",
        "    y_test.append([0,0,0,0,0,1,0,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"sandal\") != -1:\n",
        "    y_test.append([0,0,0,0,0,0,1,0,0,0])\n",
        "\n",
        "  elif test_path.find(\"shirt\") != -1:\n",
        "    y_test.append([0,0,0,0,0,0,0,1,0,0])\n",
        "\n",
        "  elif test_path.find(\"sneaker\") != -1:\n",
        "    y_test.append([0,0,0,0,0,0,0,0,1,0])\n",
        "\n",
        "  elif test_path.find(\"top\") != -1:\n",
        "    y_test.append([0,0,0,0,0,0,0,0,0,1])\n",
        "  else:\n",
        "    print(\"SOMETHING WENT WRONG!\")\n",
        "\n",
        "# We need to convert them to numpy arrays as the rest of the code relies on it.\n",
        "\n",
        "X_train = np.array(X_train)\n",
        "X_test = np.array(X_test)\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "# X_train images are of 28 X 28 pixels\n",
        "\n",
        "\n",
        "print(\"Number of training sample: \", X_train.shape[0])  # You can change len(X_train) based on your implementation such that total number of training samples is printed\n",
        "\n",
        "print(\"Number of testing sample: \", X_test.shape[0])   # You can change len(X_test) based on your implementation such that total number of test samples is printed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training sample:  2000\n",
            "Number of testing sample:  1600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hidden": true,
        "id": "QOxoQ-_B1X3B"
      },
      "source": [
        "#### __NN Implementation__\n",
        "Your implementation of NN needs to use the `sigmoid` activation function for the hidden layer(s) and the `softmax` activation function for the output layer. The NN model you will be creating here will consits of only three layers: 1 input layer, 1 hidden layer and 1 output layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2019-01-27T01:25:17.044301Z",
          "start_time": "2019-01-27T01:25:16.984346Z"
        },
        "hidden": true,
        "id": "iP5ZI2Rs1X3C"
      },
      "source": [
        "class NeuralNetwork():\n",
        "    \n",
        "    @staticmethod                    \n",
        "    def cross_entropy_loss(y_pred, y_true):  \n",
        "        ###### Code Here ######\n",
        "        ln_y_pred = np.log(y_pred)\n",
        "        ln_one_minus_y_pred = np.log(1-y_pred)\n",
        "\n",
        "        sum = -1*((y_true * ln_y_pred) + ((1-y_true)*ln_one_minus_y_pred))\n",
        "\n",
        "        sum2 = np.sum(sum)\n",
        "        return sum2/y_pred.shape[0]\n",
        "        \n",
        "  \n",
        "    @staticmethod\n",
        "    def accuracy(y_pred, y_true):\n",
        "        ###### Code Here ######\n",
        "        \n",
        "        correctly_classified = 0\n",
        "\n",
        "        for pred,true in zip(y_pred,y_true):\n",
        "          if pred == true:\n",
        "            correctly_classified += 1\n",
        "\n",
        "        \n",
        "        return correctly_classified*100.0/len(y_true)\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        ###### Code Here ######\n",
        "\n",
        "        # calculate the vector found by exponentiating\n",
        "        x = np.exp(x)  \n",
        "\n",
        "        # Sum up all its values\n",
        "        sum = np.sum(x)\n",
        "\n",
        "\n",
        "        # softmax definition (e^zi/sum(e^zi)\n",
        "        return x/sum\n",
        "    \n",
        "    \n",
        "    @staticmethod\n",
        "    def sigmoid(x):\n",
        "        ###### Code Here ######\n",
        "        \n",
        "        return 1.0/(1.0+np.exp(-x))\n",
        "                            \n",
        "    def __init__(self, input_size, hidden_nodes, output_size):\n",
        "        '''Creates a Feed-Forward Neural Network.\n",
        "        The parameters represent the number of nodes in each layer (total 3). \n",
        "        Look at the inputs to the function'''\n",
        "        \n",
        "        self.num_layers = 3\n",
        "        self.input_shape = input_size\n",
        "        self.hidden_shape = hidden_nodes\n",
        "        self.output_shape = output_size\n",
        "        \n",
        "        self.weights_ = []\n",
        "        self.biases_ = []\n",
        "        self.__init_weights()\n",
        "    \n",
        "    def __init_weights(self):\n",
        "        '''Initializes all weights based on standard normal distribution and all biases to 0.'''\n",
        "        \n",
        "        ###### Code Here (Replace 'None' by appropriate values/varaibles) ######\n",
        "        \n",
        "        \n",
        "\n",
        "\n",
        "        W_h = np.random.normal(size=(self.input_shape,self.hidden_shape))   \n",
        "        b_h = np.zeros(shape=(1,self.hidden_shape))                         # This is a 2-D numpy array of shape (1,n) for later use in forward_pass function.\n",
        "\n",
        "        W_o = np.random.normal(size=(self.hidden_shape,self.output_shape))   \n",
        "        b_o = np.zeros(shape=(1,self.output_shape))                          # This is a 2-D numpy array of shape (1,n) for later use in forward_pass function.\n",
        "        \n",
        "        # self.weights_ becomes a list of np.arrays. 0th index has W_h and 1st index has W_o\n",
        "        self.weights_.append(W_h)  \n",
        "        self.weights_.append(W_o)  \n",
        "\n",
        "        # self.biases_ becomes a list of np.arrays. 0th index has b_h and 1st index has b_o\n",
        "        self.biases_.append(b_h)\n",
        "        self.biases_.append(b_o)\n",
        "\n",
        "\n",
        "\n",
        "                          \n",
        "    def forward_pass(self, input_data):\n",
        "        '''Executes the feed forward algorithm.\n",
        "        \"input_data\" is the input to the network in row-major form\n",
        "        Returns \"activations\", which is a list of all layer outputs (excluding input layer of course)'''\n",
        "\n",
        "        ###### Code Here ######\n",
        "        activations = []\n",
        "\n",
        "\n",
        "        sum = 0\n",
        "        dot = np.empty((1,self.hidden_shape))\n",
        "        cols = self.weights_[0].shape[1]\n",
        "        for i in range(0,cols):\n",
        "          for j in range(0,self.input_shape):\n",
        "            sum = sum + self.weights_[0][j,i]*input_data[0,j]     # for loop to help in finding activations in hidden layer (basically wa)\n",
        "          dot[0,i] = sum\n",
        "          sum = 0\n",
        "        activation_hidden_layer = self.sigmoid(dot + self.biases_[0])  #adding bias to get activations (a_new = wa + b)\n",
        "\n",
        "        sum = 0\n",
        "        dot = np.empty((1,self.output_shape))\n",
        "        cols = self.weights_[1].shape[1]\n",
        "        for i in range(0,cols):\n",
        "          for j in range(0,self.hidden_shape):\n",
        "            sum = sum + self.weights_[1][j,i]*activation_hidden_layer[0,j]   #Same for next layer\n",
        "          dot[0,i] = sum\n",
        "          sum = 0\n",
        "        activation_output_layer = self.softmax(dot + self.biases_[1])\n",
        "\n",
        "        activations.append(activation_hidden_layer)\n",
        "        activations.append(activation_output_layer)\n",
        "\n",
        "        \n",
        "        return activations            # List has to be returned, not numpy array (will give error + even mentioned in the prompt above)\n",
        "        #The list returned contains two 2-D (1,n) numpy arrays. Don't modify this. It's supposed to be this way.\n",
        "\n",
        "                          \n",
        "    def backward_pass(self, targets, layer_activations):\n",
        "        '''Executes the backpropogation algorithm.\n",
        "        \"targets\" is the ground truth/labels\n",
        "        \"layer_activations\" are the return value of the forward pass step\n",
        "        Returns \"deltas\", which is a list containing weight update values for all layers (excluding the input layer of course)'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "\n",
        "\n",
        "        gradient = -1*(targets/layer_activations[1] - (1-targets)/(1-layer_activations[1]))   # Calculating del Cost to backpropagate to hidden layer\n",
        "        soft_derivative1 = layer_activations[1]*(1-layer_activations[1])                      # Calculating derivative of softmax\n",
        "        output_error = gradient*soft_derivative1                                              # Output error given by multiplying above mentioned things\n",
        "\n",
        "        soft_derivative0 = layer_activations[0]*(1-layer_activations[0])              # derivative of sigmoid\n",
        "        deltas = []        \n",
        "\n",
        "        \n",
        "        sum = 0\n",
        "        dot = np.empty((1,self.hidden_shape))\n",
        "        cols = self.weights_[1].shape[0]\n",
        "        for i in range(0,cols):\n",
        "          for j in range(0,self.output_shape):\n",
        "            sum = sum + self.weights_[1][i,j]*output_error[0,j]                # Doing dot (matrix multiplication) product of output error with  \n",
        "          dot[0,i] = sum                                                              # weights of hidden layer to back propagate error\n",
        "          sum = 0\n",
        "        error = dot*soft_derivative0\n",
        "    \n",
        "        deltas.append(output_error)\n",
        "        deltas.append(error)\n",
        "\n",
        "        return deltas\n",
        "                           \n",
        "    def weight_update(self, deltas, layer_inputs, lr):\n",
        "        '''Executes the gradient descent algorithm.\n",
        "        \"deltas\" is return value of the backward pass step\n",
        "        \"layer_inputs\" is a list containing the inputs for all layers (including the input layer)\n",
        "        \"lr\" is the learning rate'''\n",
        "        \n",
        "        ###### Code Here ######\n",
        "\n",
        "    \n",
        "        delC_by_delW_output = np.empty((self.hidden_shape,self.output_shape))         # Creating empty matrix for weight updates\n",
        "\n",
        "        for i in range(0,self.hidden_shape):\n",
        "            delC_by_delW_output[i] = layer_inputs[1][0,i]*deltas[0][0]                # Gradient descent for output layer: dot of error with inputs\n",
        "                                                                                      # One for loop used because each row of desired array can be obtained  \n",
        "        delC_by_delB_output = deltas[0]   # Bias updates                              # by essentially multiplying each element (scalar) of one array  \n",
        "                                                                                      # with others entire row\n",
        "        self.weights_[1] = self.weights_[1] - lr*(delC_by_delW_output)   # Updating weights\n",
        "        self.biases_[1] = self.biases_[1] - lr*(delC_by_delB_output)     # Updating biases\n",
        "\n",
        "        delC_by_delW_hidden = np.empty((self.input_shape,self.hidden_shape))   # Repeat process for hidden layer\n",
        "\n",
        "        for i in range(0,self.input_shape):\n",
        "            delC_by_delW_hidden[i] = layer_inputs[0][0,i]*deltas[1][0]\n",
        "        \n",
        "        delC_by_delB_hidden = deltas[1]\n",
        "\n",
        "        self.weights_[0] = self.weights_[0] - lr*(delC_by_delW_hidden)\n",
        "        self.biases_[0] = self.biases_[0] - lr*(delC_by_delB_hidden)\n",
        "\n",
        "\n",
        "    ###### Do Not Change Anything Below this line in This Cell ######\n",
        "    \n",
        "    def fit(self, Xs, Ys, epochs, lr=1e-3):\n",
        "            history = []\n",
        "            for epoch in tqdm_notebook(range(epochs)):\n",
        "                num_samples = Xs.shape[0]\n",
        "                for i in range(num_samples):\n",
        "\n",
        "                    sample_input = Xs[i,:].reshape((1,self.input_shape))\n",
        "                    sample_target = Ys[i,:].reshape((1,self.output_shape))\n",
        "                    \n",
        "                    activations = self.forward_pass(sample_input)   # Call forward_pass function \n",
        "                    deltas = self.backward_pass(sample_target, activations)    # Call backward_pass function \n",
        "                    layer_inputs = [sample_input] + activations[:-1]\n",
        "                    \n",
        "                    # Call weight_update function \n",
        "                    self.weight_update(deltas, layer_inputs, lr)\n",
        "                \n",
        "                preds = self.predict(Xs)   # Call predict function \n",
        "\n",
        "                current_loss = self.cross_entropy_loss(preds, Ys)\n",
        "                \n",
        "                if  epoch==epochs-1:\n",
        "                  confusion_mat=confusion_matrix(Ys.argmax(axis=1), preds.argmax(axis=1),labels=np.arange(10))  \n",
        "                  plot_confusion_matrix(confusion_mat)\n",
        "                  report = classification_report(Ys, np_utils.to_categorical(preds.argmax(axis=1),num_classes=classes), target_names=class_labels)\n",
        "                  print(report)\n",
        "                history.append(current_loss)\n",
        "            return history\n",
        "\n",
        "    def predict(self, Xs):\n",
        "        '''Returns the model predictions (output of the last layer) for the given \"Xs\".'''\n",
        "        predictions = []\n",
        "        num_samples = Xs.shape[0]\n",
        "        for i in range(num_samples):\n",
        "            sample = Xs[i,:].reshape((1,self.input_shape))\n",
        "            sample_prediction = self.forward_pass(sample)[-1]\n",
        "            predictions.append(sample_prediction.reshape((self.output_shape,)))\n",
        "        return np.array(predictions)\n",
        "\n",
        "    def evaluate(self, Xs, Ys):\n",
        "        '''Returns appropriate metrics for the task, calculated on the dataset passed to this method.'''\n",
        "        pred = self.predict(Xs)\n",
        "        return self.cross_entropy_loss(pred, Ys), self.accuracy(pred.argmax(axis=1), Ys.argmax(axis=1))\n",
        "  \n",
        "    def plot_model(self, filename):\n",
        "        '''Provide the \"filename\" as a string including file extension. Creates an image showing the model as a graph.'''\n",
        "        graph = pydot.Dot(graph_type='digraph')\n",
        "        graph.set_rankdir('LR')\n",
        "        graph.set_node_defaults(shape='circle', fontsize=0)\n",
        "        nodes_per_layer = [self.input_shape, self.hidden_shape, self.output_shape]\n",
        "        for i in range(self.num_layers-1):\n",
        "            for n1 in range(nodes_per_layer[i]):\n",
        "                for n2 in range(nodes_per_layer[i+1]):\n",
        "                    edge = pydot.Edge(f'l{i}n{n1}', f'l{i+1}n{n2}')\n",
        "                    graph.add_edge(edge)\n",
        "        graph.write_png(filename)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_200Z0dwE2Fq"
      },
      "source": [
        "# These are what we call the hyperparameters (a.k.a Black Magic). You need to research on them and tweak them to see what generates the best result for you \n",
        "\n",
        "# This is because there are 784 pixels in that image already\n",
        "INPUT_SIZE = 784        # must be an int, this number represents the numeber of nodes/neurons in the input layer of the network\n",
        "\n",
        "HIDDEN_NODES = 15     # must be an int, this number represents the numeber of nodes/neurons in the only hidden layer of the network\n",
        "OUTPUT_SIZE = 10      # must be an int, this number represents the numeber of nodes/neurons in the output layer of the network\n",
        "EPOCHS = 100         # must be an int\n",
        "LEARNING_RATE = 1e-3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRtc2Dzc2D_H",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "2d4395c702e040bbb8e69d0e7d7a9aee",
            "fc9c06978742439585f0332a2d2c2f0a",
            "a6af3879353b4ba5b134fa7fb28e6272",
            "093427a057984477801baddd526f23d7",
            "d15df66a2a584dd38fbdfbc11e43e579",
            "a891a5af58d2437bbd0db564ede7b94a",
            "f089289a774a40e4a5d7651a20b15b77",
            "85a34d4be9e84e708f0c151fdf069ac1"
          ]
        },
        "outputId": "c16d087b-9815-4096-fdce-b6dbf0b6818b"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "nn = NeuralNetwork(input_size = INPUT_SIZE, hidden_nodes = HIDDEN_NODES, output_size = OUTPUT_SIZE)\n",
        "history = nn.fit(X_train, y_train, epochs=EPOCHS, lr=LEARNING_RATE)\n",
        "plt.plot(history);\n",
        "plt.gca().set(xlabel='Epoch', ylabel='Cross-entropy', title='Training Plot {}'.format(rollnumber));\n",
        "end = time.time()\n",
        "\n",
        "print(\"Runtime of the algorithm is \", round((end - start),3),\" seconds\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:192: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
            "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2d4395c702e040bbb8e69d0e7d7a9aee",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4AgT8as5IX1"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lF4fjsgS4YDo"
      },
      "source": [
        "nn.evaluate(X_test,y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}